{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a vector of input scores (logits) $ \\mathbf{z} = (z_1, z_2, ..., z_d) \\in \\mathbb{R}^d $, the α-entmax transformation computes a probability distribution $ \\mathbf{p} = \\alpha\\text{-entmax}(\\mathbf{z}) $ where $ \\mathbf{p} \\in \\Delta^d $ (the probability simplex).\n",
    "\n",
    "For $ \\alpha \\ge 1 $, the formula for the $ i $-th component of the output probability vector $ \\mathbf{p} $ is:\n",
    "\n",
    "$ p_i = \\alpha\\text{-entmax}_i(\\mathbf{z}) = \\left[ (\\alpha - 1) z_i - \\tau(\\mathbf{z}) \\right]_+^{1/(\\alpha-1)} $\n",
    "\n",
    "Where:\n",
    "\n",
    "$ \\alpha $: The hyperparameter controlling the sparsity. $ \\alpha=1 $ recovers Softmax, $ \\alpha=2 $ recovers Sparsemax. For $ \\alpha > 1 $, the transformation can produce sparse outputs (exact zeros). Your code tested $ \\alpha = 1.0, 1.5, 2.0 $.\n",
    "$ z_i $: The $ i $-th input score (logit).\n",
    "$ [x]_+ $: This denotes the positive part function (ReLU), i.e., $ [x]_+ = \\max{x, 0} $. This is what allows the output probabilities to be exactly zero for low scores when $ \\alpha > 1 $.\n",
    "$ \\tau(\\mathbf{z}) $: This is a threshold value (a Lagrange multiplier) that is determined uniquely for each input vector $ \\mathbf{z} $ such that the resulting vector $ \\mathbf{p} $ sums to 1, i.e., $ \\sum_{j=1}^d p_j = 1 $. Finding this $ \\tau $ efficiently is often the main challenge in computing α-entmax, and methods like bisection search (as used in entmax_bisect) are employed.\n",
    "In simple terms:\n",
    "\n",
    "The input logits $ z_i $ are scaled by $ (\\alpha - 1) $.\n",
    "A threshold $ \\tau $ (specific to the input vector $ \\mathbf{z} $) is subtracted.\n",
    "The result is passed through a ReLU function $ [\\cdot]_+ $ to zero out negative values.\n",
    "The non-zero results are raised to the power of $ 1/(\\alpha-1) $.\n",
    "The threshold $ \\tau $ is chosen precisely so that these final values sum to 1, forming a valid (potentially sparse) probability distribution.\n",
    "\n",
    "\n",
    "Metrics\n",
    "\n",
    "## Result and Analysis (Run result below)\n",
    "\n",
    "| Alpha ($ \\alpha $) | Temperature (T) | Val Loss | Utilization | Imbalance | Concentration | TPE CV | Avg Prob CV |\n",
    "|--------------------|-----------------|----------|-------------|-----------|---------------|--------|-------------|\n",
    "| 1                  | 0.5             | 2.3981   | 1           | 1         | 0.125         | 0      | 0           |\n",
    "| 1                  | 1               | 1.618    | 1           | 1         | 0.125         | 0      | 0           |\n",
    "| 1                  | 10              | 1.5866   | 1           | 1         | 0.125         | 0      | 0           |\n",
    "| 1.5                | 0.5             | 1.5641   | 1           | 1.6958    | 0.8997        | 0.3955 | 0.5014      |\n",
    "| 1.5                | 1               | 1.5604   | 1           | 1.2568    | 0.7286        | 0.1769 | 0.2747      |\n",
    "| 1.5                | 10              | 1.5752   | 1           | 1.1047    | 0.4309        | 0.0793 | 0.2631      |\n",
    "| 2                  | 0.5             | 1.5702   | 1           | 2.0718    | 0.9728        | 0.5883 | 0.6088      |\n",
    "| 2                  | 1               | 1.5624   | 1           | 1.7956    | 0.8461        | 0.4227 | 0.4705      |\n",
    "| 2                  | 10              | 1.5622   | 1           | 1.188     | 0.437         | 0.133  | 0.1614      |\n",
    "\n",
    "### Best Performance (Validation Loss):\n",
    "\n",
    "- The lowest validation loss (best performance) was achieved with alpha=1.5, temperature=1.0 (Val Loss: 1.5604).\n",
    "- Two other configurations were very close:\n",
    "    - alpha=2.0, temperature=10.0 (Val Loss: 1.5622)\n",
    "    - alpha=2.0, temperature=1.0 (Val Loss: 1.5624)\n",
    "- The models with alpha=1.0 performed significantly worse, especially at low temperature (temp=0.5) -> the standard Softmax (alpha=1.0) routing wasn't optimal for this setup.\n",
    "\n",
    "\n",
    "### Effect of Alpha ($ \\alpha $): (Controls sparsity function shape; higher alpha promotes sparser outputs)\n",
    "\n",
    "\n",
    "- As expected, increasing alpha generally increased Concentration (average max probability per token) and Imbalance (ratio of max expert load to mean expert load). This indicates sparser, more focused expert selection, but potentially worse load balancing.\n",
    "\n",
    "At alpha=1.0 (Softmax), Concentration was minimal (0.125, i.e., 1/num_experts), and Imbalance was perfect (1.0). This means Softmax distributed tokens evenly but didn't achieve any routing sparsity.\n",
    "alpha=2.0 produced the highest concentration and imbalance, especially at lower temperatures. This indicates very sparse routing (most probability mass on one expert), leading to higher load imbalance.\n",
    "\n",
    "### Effect of Temperature (T): (Scales logits before sparsity function; lower T sharpens, higher T softens)\n",
    "\n",
    "- Lower temperatures (T=0.5) significantly increased Concentration and Imbalance for alpha > 1.0. This sharpening effect combined with sparser functions (alpha=1.5 or 2.0) led to very high concentration and poor load balancing.\n",
    "- Higher temperatures (T=10.0) softened the distributions, resulting in lower Concentration and Imbalance compared to T=1.0 or T=0.5 for the same alpha.\n",
    "- Interestingly, for alpha=1.0 (Softmax), temperature had no effect on the final metrics. This is because Softmax inherently produces dense outputs regardless of temperature scaling (unless T is extremely low).\n",
    "- The very poor performance of alpha=1.0, temp=0.5 might be due to the temperature being too low, causing instability or difficulty during training when combined with standard Softmax.\n",
    "\n",
    "\n",
    "### MoE Metrics (Utilization, CVs):\n",
    "\n",
    "* Utilization was 1.0 (100%) for almost all runs, meaning all experts received at least some tokens during evaluation. This is good.\n",
    "* TPE CV (Coefficient of Variation for Tokens Per Expert) and Avg Prob CV (CV for Average Expert Probability) generally followed the trends of Imbalance and Concentration. Higher values indicate more uneven distribution of tokens or routing probabilities across experts.\n",
    "* The lowest CV values (most balanced) were for alpha=1.0 (perfectly balanced) and generally increased with alpha and decreased with temperature.\n",
    "\n",
    "\n",
    "#### Some Trade-offs:\n",
    "\n",
    "* Best Performance: The sweet spot for validation loss seems to be around alpha=1.5, temp=1.0 or alpha=2.0 with temp=1.0 or temp=10.0.\n",
    "* **Sparsity vs. Performance**: Using alpha=1.5 or alpha=2.0 clearly improved performance over standard Softmax (alpha=1.0). This indicates that sparse routing was beneficial.\n",
    "* **Temperature Tuning**: Temperature is crucial.\n",
    "    * Very low temperatures (T=0.5) combined with alpha > 1.0 led to extremely high concentration and imbalance, which slightly hurt performance compared to T=1.0.\n",
    "    * High temperature (T=10.0) seemed effective at mitigating the imbalance caused by high alpha (alpha=2.0, temp=10.0 performed well).\n",
    "* **Load Balancing**: There's a trade-off. Achieving the best validation loss involved accepting some degree of imbalance (values around 1.2-1.8 for the top models, compared to 1.0 for Softmax).\n",
    "* The configuration with the highest imbalance (alpha=2.0, temp=0.5) did not have the best loss.\n",
    "* The combination alpha=1.5, temperature=1.0 stands out as the best performer with reasonably balanced MoE metrics compared to the other high-performing, high-alpha configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:35:43.613398Z",
     "iopub.status.busy": "2025-04-18T10:35:43.613057Z",
     "iopub.status.idle": "2025-04-18T10:35:43.620719Z",
     "shell.execute_reply": "2025-04-18T10:35:43.619992Z",
     "shell.execute_reply.started": "2025-04-18T10:35:43.613374Z"
    }
   },
   "outputs": [],
   "source": [
    "input_path = '/kaggle/input/input-txt/input.txt'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experts and Routers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:34:05.563805Z",
     "iopub.status.busy": "2025-04-18T10:34:05.563499Z",
     "iopub.status.idle": "2025-04-18T10:34:05.568060Z",
     "shell.execute_reply": "2025-04-18T10:34:05.567084Z",
     "shell.execute_reply.started": "2025-04-18T10:34:05.563778Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:37:45.274460Z",
     "iopub.status.busy": "2025-04-18T10:37:45.274132Z",
     "iopub.status.idle": "2025-04-18T10:37:45.281366Z",
     "shell.execute_reply": "2025-04-18T10:37:45.280480Z",
     "shell.execute_reply.started": "2025-04-18T10:37:45.274432Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from entmax import entmax_bisect\n",
    "from sparsemax import Sparsemax\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "sns.set_palette('pastel')\n",
    "sns.set_style('whitegrid')\n",
    "import numpy as np\n",
    "batch_size = 128\n",
    "block_size = 32\n",
    "max_iters = 3000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 400\n",
    "head_size = 16\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "num_experts = 8\n",
    "setup_seed\n",
    "device = 'cuda' if torch.cuda.is_available() else (\n",
    "    'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:35:47.034565Z",
     "iopub.status.busy": "2025-04-18T10:35:47.034075Z",
     "iopub.status.idle": "2025-04-18T10:35:47.040349Z",
     "shell.execute_reply": "2025-04-18T10:35:47.039425Z",
     "shell.execute_reply.started": "2025-04-18T10:35:47.034518Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "setup_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:37:47.144741Z",
     "iopub.status.busy": "2025-04-18T10:37:47.144460Z",
     "iopub.status.idle": "2025-04-18T10:37:47.335550Z",
     "shell.execute_reply": "2025-04-18T10:37:47.334855Z",
     "shell.execute_reply.started": "2025-04-18T10:37:47.144720Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Vocab size: {vocab_size}\\tData length: {len(text)}\")\n",
    "print(f\"Train data length: {len(train_data)}\\tValidation data length: {len(val_data)}\")\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data_source = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_source[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data_source[i + 1:i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embed, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert n_embed % num_heads == 0\n",
    "        head_size = n_embed // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class AlphaEntmaxRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, alpha=1.5, temperature=1.0, n_iter=50):\n",
    "        super(AlphaEntmaxRouter, self).__init__()\n",
    "        assert temperature > 1e-9\n",
    "        self.num_experts = num_experts\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.n_iter = n_iter\n",
    "        self.route_linear = nn.Linear(n_embed, num_experts)\n",
    "        print(f\"Initialized AlphaEntmaxRouter (functional) with alpha = {self.alpha}, temperature = {self.temperature}\")\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        logits = self.route_linear(mh_output)\n",
    "        scaled_logits = logits / self.temperature\n",
    "        try:\n",
    "            router_output = entmax_bisect(scaled_logits, alpha=self.alpha, dim=-1, n_iter=self.n_iter)\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: entmax_bisect forward failed. Error: {e}. Falling back to Softmax.\")\n",
    "            router_output = F.softmax(scaled_logits, dim=-1)\n",
    "        return router_output\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, dropout, router_alpha=1.5, router_temperature=1.0):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = AlphaEntmaxRouter(n_embed, num_experts, alpha=router_alpha, temperature=router_temperature)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed, dropout) for _ in range(num_experts)])\n",
    "        self.num_experts = num_experts\n",
    "        self.register_buffer('latest_tpe', torch.zeros(num_experts, dtype=torch.float32))\n",
    "        self.latest_imbalance = 0.0\n",
    "        self.latest_concentration = 0.0\n",
    "        self.latest_utilization = 0.0\n",
    "        self.latest_tpe_cv = 0.0\n",
    "        self.latest_avg_prob_cv = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "        gating_output = self.router(x)\n",
    "        with torch.no_grad():\n",
    "            flat_gating_output_no_grad = gating_output.reshape(-1, self.num_experts).detach()\n",
    "            is_active = flat_gating_output_no_grad > 1e-9\n",
    "            tpe = is_active.sum(dim=0).float()\n",
    "            self.latest_tpe = tpe\n",
    "            if self.num_experts > 1:\n",
    "                mean_tpe = tpe.mean()\n",
    "                std_tpe = tpe.std(unbiased=False)\n",
    "                self.latest_tpe_cv = (std_tpe / (mean_tpe + 1e-9)).item() if mean_tpe > 1e-9 else 0.0\n",
    "            else:\n",
    "                self.latest_tpe_cv = 0.0\n",
    "            mean_tpe_val = tpe.mean().item()\n",
    "            if self.num_experts > 0 and mean_tpe_val > 0:\n",
    "                self.latest_imbalance = tpe.max().item() / (mean_tpe_val + 1e-9)\n",
    "            else:\n",
    "                self.latest_imbalance = 1.0\n",
    "            max_p_per_token, _ = flat_gating_output_no_grad.max(dim=-1)\n",
    "            self.latest_concentration = max_p_per_token.mean().item() if num_tokens > 0 else 0.0\n",
    "            num_active_experts = (tpe > 0).sum().item()\n",
    "            self.latest_utilization = num_active_experts / self.num_experts if self.num_experts > 0 else 0.0\n",
    "            if self.num_experts > 1 and num_tokens > 0:\n",
    "                avg_prob = flat_gating_output_no_grad.mean(dim=0)\n",
    "                mean_avg_prob = avg_prob.mean()\n",
    "                std_avg_prob = avg_prob.std(unbiased=False)\n",
    "                self.latest_avg_prob_cv = (\n",
    "                            std_avg_prob / (mean_avg_prob + 1e-9)).item() if mean_avg_prob > 1e-9 else 0.0\n",
    "            else:\n",
    "                self.latest_avg_prob_cv = 0.0\n",
    "        flat_x = x.reshape(-1, n_embed)\n",
    "        final_output = torch.zeros_like(flat_x)\n",
    "        flat_gating_output_for_weighting = gating_output.reshape(-1, self.num_experts)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_scores = flat_gating_output_for_weighting[:, i]\n",
    "            token_indices = torch.nonzero(flat_gating_output_no_grad[:, i] > 1e-9).squeeze(-1)\n",
    "            if token_indices.numel() == 0: continue\n",
    "            expert_input = flat_x[token_indices]\n",
    "            active_gating_scores = expert_scores[token_indices].unsqueeze(1)\n",
    "            expert_output = expert(expert_input)\n",
    "            weighted_output = expert_output * active_gating_scores\n",
    "            final_output.index_add_(0, token_indices, weighted_output)\n",
    "        final_output = final_output.view(batch_size, seq_len, n_embed)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, num_experts, block_size, dropout, router_alpha=1.5, router_temperature=1.0):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.sa = MultiHeadAttention(n_embed, n_head, block_size, dropout)\n",
    "        self.smoe = SparseMoE(n_embed, num_experts, dropout, router_alpha=router_alpha,\n",
    "                              router_temperature=router_temperature)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SparseMoELanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer, num_experts, block_size, dropout, router_alpha=1.5,\n",
    "                 router_temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(n_embed=n_embed, n_head=n_head, num_experts=num_experts, block_size=block_size, dropout=dropout,\n",
    "                  router_alpha=router_alpha, router_temperature=router_temperature) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        print(f\"Initialized SparseMoELanguageModel with {n_layer} layers.\")\n",
    "        print(f\"Router settings per layer: alpha={router_alpha}, temperature={router_temperature}\")\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_for_loss = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_for_loss, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {'loss': {}}\n",
    "    num_model_layers = n_layer\n",
    "    accumulated_metrics = {\n",
    "        f'L{i}': {'imbalance': 0.0, 'concentration': 0.0, 'utilization': 0.0, 'tpe_cv': 0.0, 'avg_prob_cv': 0.0,\n",
    "                  'count': 0}\n",
    "        for i in range(num_model_layers)\n",
    "    }\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(num_model_layers):\n",
    "            for key in accumulated_metrics[f'L{i}']:\n",
    "                accumulated_metrics[f'L{i}'][key] = 0.0 if key != 'count' else 0\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "            if hasattr(model, 'blocks') and isinstance(model.blocks, nn.Sequential):\n",
    "                for i, block in enumerate(model.blocks):\n",
    "                    if isinstance(block, Block) and hasattr(block, 'smoe') and isinstance(block.smoe, SparseMoE):\n",
    "                        smoe_layer = block.smoe\n",
    "                        layer_key = f'L{i}'\n",
    "                        if num_experts > 0:\n",
    "                            accumulated_metrics[layer_key]['imbalance'] += smoe_layer.latest_imbalance\n",
    "                            accumulated_metrics[layer_key]['concentration'] += smoe_layer.latest_concentration\n",
    "                            accumulated_metrics[layer_key]['utilization'] += smoe_layer.latest_utilization\n",
    "                            accumulated_metrics[layer_key]['tpe_cv'] += smoe_layer.latest_tpe_cv\n",
    "                            accumulated_metrics[layer_key]['avg_prob_cv'] += smoe_layer.latest_avg_prob_cv\n",
    "                            accumulated_metrics[layer_key]['count'] += 1\n",
    "        out['loss'][split] = losses.mean()\n",
    "    averaged_metrics = {f'L{i}': {} for i in range(num_model_layers)}\n",
    "    for i in range(num_model_layers):\n",
    "        layer_key = f'L{i}'\n",
    "        count = accumulated_metrics[layer_key]['count']\n",
    "        if count > 0:\n",
    "            for key in accumulated_metrics[layer_key]:\n",
    "                if key != 'count':\n",
    "                    averaged_metrics[layer_key][key] = accumulated_metrics[layer_key][key] / count\n",
    "        else:\n",
    "            for key in accumulated_metrics[layer_key]:\n",
    "                if key != 'count': averaged_metrics[layer_key][key] = 0.0\n",
    "    model.train()\n",
    "    return out['loss'], averaged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:35:57.207773Z",
     "iopub.status.busy": "2025-04-18T10:35:57.207483Z",
     "iopub.status.idle": "2025-04-18T10:35:57.212988Z",
     "shell.execute_reply": "2025-04-18T10:35:57.212119Z",
     "shell.execute_reply.started": "2025-04-18T10:35:57.207752Z"
    }
   },
   "outputs": [],
   "source": [
    "setup_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:37:26.941310Z",
     "iopub.status.busy": "2025-04-18T10:37:26.940972Z",
     "iopub.status.idle": "2025-04-18T10:37:27.108290Z",
     "shell.execute_reply": "2025-04-18T10:37:27.107413Z",
     "shell.execute_reply.started": "2025-04-18T10:37:26.941284Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_batch(split):\n",
    "    data_source = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_source[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data_source[i + 1:i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embed, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert n_embed % num_heads == 0\n",
    "        head_size = n_embed // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class AlphaEntmaxRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, alpha=1.5, temperature=1.0, n_iter=50):\n",
    "        super(AlphaEntmaxRouter, self).__init__()\n",
    "        assert temperature > 1e-9\n",
    "        self.num_experts = num_experts\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.n_iter = n_iter\n",
    "        self.route_linear = nn.Linear(n_embed, num_experts)\n",
    "        print(f\"Initialized AlphaEntmaxRouter (functional) with alpha = {self.alpha}, temperature = {self.temperature}\")\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        logits = self.route_linear(mh_output)\n",
    "        scaled_logits = logits / self.temperature\n",
    "        try:\n",
    "            router_output = entmax_bisect(scaled_logits, alpha=self.alpha, dim=-1, n_iter=self.n_iter)\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: entmax_bisect forward failed. Error: {e}. Falling back to Softmax.\")\n",
    "            router_output = F.softmax(scaled_logits, dim=-1)\n",
    "        return router_output\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, dropout, router_alpha=1.5, router_temperature=1.0):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = AlphaEntmaxRouter(n_embed, num_experts, alpha=router_alpha, temperature=router_temperature)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed, dropout) for _ in range(num_experts)])\n",
    "        self.num_experts = num_experts\n",
    "        self.register_buffer('latest_tpe', torch.zeros(num_experts, dtype=torch.float32))\n",
    "        self.latest_imbalance = 0.0\n",
    "        self.latest_concentration = 0.0\n",
    "        self.latest_utilization = 0.0\n",
    "        self.latest_tpe_cv = 0.0\n",
    "        self.latest_avg_prob_cv = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "        gating_output = self.router(x)\n",
    "        with torch.no_grad():\n",
    "            flat_gating_output_no_grad = gating_output.reshape(-1, self.num_experts).detach()\n",
    "            is_active = flat_gating_output_no_grad > 1e-9\n",
    "            tpe = is_active.sum(dim=0).float()\n",
    "            self.latest_tpe = tpe\n",
    "            if self.num_experts > 1:\n",
    "                mean_tpe = tpe.mean()\n",
    "                std_tpe = tpe.std(unbiased=False)\n",
    "                self.latest_tpe_cv = (std_tpe / (mean_tpe + 1e-9)).item() if mean_tpe > 1e-9 else 0.0\n",
    "            else:\n",
    "                self.latest_tpe_cv = 0.0\n",
    "            mean_tpe_val = tpe.mean().item()\n",
    "            if self.num_experts > 0 and mean_tpe_val > 0:\n",
    "                self.latest_imbalance = tpe.max().item() / (mean_tpe_val + 1e-9)\n",
    "            else:\n",
    "                self.latest_imbalance = 1.0\n",
    "            max_p_per_token, _ = flat_gating_output_no_grad.max(dim=-1)\n",
    "            self.latest_concentration = max_p_per_token.mean().item() if num_tokens > 0 else 0.0\n",
    "            num_active_experts = (tpe > 0).sum().item()\n",
    "            self.latest_utilization = num_active_experts / self.num_experts if self.num_experts > 0 else 0.0\n",
    "            if self.num_experts > 1 and num_tokens > 0:\n",
    "                avg_prob = flat_gating_output_no_grad.mean(dim=0)\n",
    "                mean_avg_prob = avg_prob.mean()\n",
    "                std_avg_prob = avg_prob.std(unbiased=False)\n",
    "                self.latest_avg_prob_cv = (\n",
    "                        std_avg_prob / (mean_avg_prob + 1e-9)).item() if mean_avg_prob > 1e-9 else 0.0\n",
    "            else:\n",
    "                self.latest_avg_prob_cv = 0.0\n",
    "        flat_x = x.reshape(-1, n_embed)\n",
    "        final_output = torch.zeros_like(flat_x)\n",
    "        flat_gating_output_for_weighting = gating_output.reshape(-1, self.num_experts)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_scores = flat_gating_output_for_weighting[:, i]\n",
    "            token_indices = torch.nonzero(flat_gating_output_no_grad[:, i] > 1e-9).squeeze(-1)\n",
    "            if token_indices.numel() == 0: continue\n",
    "            expert_input = flat_x[token_indices]\n",
    "            active_gating_scores = expert_scores[token_indices].unsqueeze(1)\n",
    "            expert_output = expert(expert_input)\n",
    "            weighted_output = expert_output * active_gating_scores\n",
    "            final_output.index_add_(0, token_indices, weighted_output)\n",
    "        final_output = final_output.view(batch_size, seq_len, n_embed)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, num_experts, block_size, dropout, router_alpha=1.5, router_temperature=1.0):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.sa = MultiHeadAttention(n_embed, n_head, block_size, dropout)\n",
    "        self.smoe = SparseMoE(n_embed, num_experts, dropout, router_alpha=router_alpha,\n",
    "                              router_temperature=router_temperature)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SparseMoELanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer, num_experts, block_size, dropout, router_alpha=1.5,\n",
    "                 router_temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(n_embed=n_embed, n_head=n_head, num_experts=num_experts, block_size=block_size, dropout=dropout,\n",
    "                  router_alpha=router_alpha, router_temperature=router_temperature) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        print(f\"Initialized SparseMoELanguageModel with {n_layer} layers.\")\n",
    "        print(f\"Router settings per layer: alpha={router_alpha}, temperature={router_temperature}\")\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_for_loss = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_for_loss, targets)\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {'loss': {}}\n",
    "    num_model_layers = n_layer\n",
    "    accumulated_metrics = {\n",
    "        f'L{i}': {'imbalance': 0.0, 'concentration': 0.0, 'utilization': 0.0, 'tpe_cv': 0.0, 'avg_prob_cv': 0.0,\n",
    "                  'count': 0}\n",
    "        for i in range(num_model_layers)\n",
    "    }\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(num_model_layers):\n",
    "            for key in accumulated_metrics[f'L{i}']:\n",
    "                accumulated_metrics[f'L{i}'][key] = 0.0 if key != 'count' else 0\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "            if hasattr(model, 'blocks') and isinstance(model.blocks, nn.Sequential):\n",
    "                for i, block in enumerate(model.blocks):\n",
    "                    if isinstance(block, Block) and hasattr(block, 'smoe') and isinstance(block.smoe, SparseMoE):\n",
    "                        smoe_layer = block.smoe\n",
    "                        layer_key = f'L{i}'\n",
    "                        if num_experts > 0:\n",
    "                            accumulated_metrics[layer_key]['imbalance'] += smoe_layer.latest_imbalance\n",
    "                            accumulated_metrics[layer_key]['concentration'] += smoe_layer.latest_concentration\n",
    "                            accumulated_metrics[layer_key]['utilization'] += smoe_layer.latest_utilization\n",
    "                            accumulated_metrics[layer_key]['tpe_cv'] += smoe_layer.latest_tpe_cv\n",
    "                            accumulated_metrics[layer_key]['avg_prob_cv'] += smoe_layer.latest_avg_prob_cv\n",
    "                            accumulated_metrics[layer_key]['count'] += 1\n",
    "        out['loss'][split] = losses.mean()\n",
    "    averaged_metrics = {f'L{i}': {} for i in range(num_model_layers)}\n",
    "    for i in range(num_model_layers):\n",
    "        layer_key = f'L{i}'\n",
    "        count = accumulated_metrics[layer_key]['count']\n",
    "        if count > 0:\n",
    "            for key in accumulated_metrics[layer_key]:\n",
    "                if key != 'count':\n",
    "                    averaged_metrics[layer_key][key] = accumulated_metrics[layer_key][key] / count\n",
    "        else:\n",
    "            for key in accumulated_metrics[layer_key]:\n",
    "                if key != 'count': averaged_metrics[layer_key][key] = 0.0\n",
    "    model.train()\n",
    "    return out['loss'], averaged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:43:06.478212Z",
     "iopub.status.busy": "2025-04-18T10:43:06.477875Z",
     "iopub.status.idle": "2025-04-18T18:57:28.808824Z",
     "shell.execute_reply": "2025-04-18T18:57:28.807733Z",
     "shell.execute_reply.started": "2025-04-18T10:43:06.478182Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1.0, 1.5, 2.0]\n",
    "temperatures = [0.5, 1, 10.0]\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Data length: {len(text)}\")\n",
    "\n",
    "df_results = pd.DataFrame([], columns=['model', 'alpha', 'temperature', 'iter', 'loss_train', 'loss_val', 'utilization',\n",
    "                                       'imbalance', 'concentration', 'tpe_cv', 'avg_prob_cv'])\n",
    "\n",
    "start_time = time.time()\n",
    "for alpha in alphas:\n",
    "    print(df_results.head(2))\n",
    "    for temperature in temperatures:\n",
    "        key = f\"alpha_{alpha}_temp_{temperature}\"\n",
    "        model = SparseMoELanguageModel(\n",
    "            vocab_size=vocab_size, n_embed=n_embed, n_head=n_head, n_layer=n_layer,\n",
    "            num_experts=num_experts, block_size=block_size, dropout=dropout,\n",
    "            router_alpha=alpha, router_temperature=temperature\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        print(f'{sum(p.numel() for p in model.parameters()) / 1e6:.2f} M parameters')\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        print(f\"\\n----- {key} -----\")\n",
    "        for iter in range(max_iters):\n",
    "            if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                losses, metrics = estimate_loss(model)\n",
    "                loss_train = losses['train'].detach().cpu().numpy()\n",
    "                loss_val = losses['val'].detach().cpu().numpy()\n",
    "                avg_utilization = sum(metrics[f'L{i}']['utilization'] for i in range(n_layer)) / n_layer\n",
    "                avg_imbalance = sum(metrics[f'L{i}']['imbalance'] for i in range(n_layer)) / n_layer\n",
    "                avg_concentration = sum(metrics[f'L{i}']['concentration'] for i in range(n_layer)) / n_layer\n",
    "                avg_tpe_cv = sum(metrics[f'L{i}']['tpe_cv'] for i in range(n_layer)) / n_layer\n",
    "                avg_avg_prob_cv = sum(metrics[f'L{i}']['avg_prob_cv'] for i in range(n_layer)) / n_layer\n",
    "\n",
    "                df_results = pd.concat([df_results if not df_results.empty else None, pd.DataFrame([{\n",
    "                    'model': key,\n",
    "                    'alpha': alpha,\n",
    "                    'temperature': temperature,\n",
    "                    'iter': iter,\n",
    "                    'loss_train': loss_train, 'loss_val': loss_val,\n",
    "                    'utilization': avg_utilization, 'imbalance': avg_imbalance,\n",
    "                    'concentration': avg_concentration, 'tpe_cv': avg_tpe_cv,\n",
    "                    'avg_prob_cv': avg_avg_prob_cv\n",
    "                }])])\n",
    "\n",
    "                print(f\"\\nSTEP: {iter:<1}/{max_iters:<5} || Train Loss: {loss_train:.4f} | Val Loss: {loss_val:.4f}\")\n",
    "                print(\n",
    "                    f\"Input Layer L0 || Utilization: {avg_utilization:<6.4f} | Imbalance: {avg_imbalance:<6.4f} | Concentration: {avg_concentration:<6.4f} | TPE CV: {avg_tpe_cv:<6.4f} | Avg Prob CV: {avg_avg_prob_cv:<6.4f}\")\n",
    "\n",
    "            xb, yb = get_batch('train')\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nTraining completed for {key}, elapsed time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-18T19:37:31.750Z",
     "iopub.execute_input": "2025-04-18T18:57:28.873523Z",
     "iopub.status.busy": "2025-04-18T18:57:28.873183Z",
     "iopub.status.idle": "2025-04-18T18:57:28.904403Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "print(\"DataFrame Info:\")\n",
    "df_results.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_results.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df_results.tail())\n",
    "\n",
    "print(\"\\nUnique Alpha values:\", df_results['alpha'].unique())\n",
    "print(\"Unique Temperature values:\", df_results['temperature'].unique())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6965849,
     "sourceId": 11163083,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
