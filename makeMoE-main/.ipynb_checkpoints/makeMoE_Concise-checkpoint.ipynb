{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e9b80fc-12cf-41a9-a0de-354f678b412b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Sparse mixture of experts language model from scratch inspired by (and largely based on) Andrej Karpathy's makemore (https://github.com/karpathy/makemore) :)\n",
    "\n",
    "This is a from scratch implementation of a sparse mixture of experts language model. This is inspired by and largely based on Andrej Karpathy's project 'makemore' and borrows most of the re-usable components from that implementation. Just like makemore, makeMoE is also an autoregressive character-level language model but uses the aforementioned sparse mixture of experts architecture. \n",
    "\n",
    "Just like makemore, pytorch is the only requirement (so I hope the from scratch claim is justified).\n",
    "\n",
    "Significant Changes from the makemore architecture\n",
    "\n",
    "- Sparse mixture of experts instead of the solitary feed forward neural net. \n",
    "- Top-k gating and noisy top-k gating implementations.\n",
    "- initialization - Kaiming He initialization is used here but the point of this notebook is to be hackable so you can swap in Xavier Glorot etc. and take it for a spin.\n",
    "\n",
    "Unchanged from makemore\n",
    "- dataset, preprocessing (tokenization), and the language modeling task Andrej chose originally - generate Shakespeare-like text\n",
    "- Casusal self attention implementation \n",
    "- Training loop\n",
    "- Inference logic\n",
    "\n",
    "Publications heavily referenced for this implementation: \n",
    "- Mixtral of experts: https://arxiv.org/pdf/2401.04088.pdf\n",
    "- Outrageosly Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts layer: https://arxiv.org/pdf/1701.06538.pdf\n",
    "\n",
    "\n",
    "** While makeMoE_from_Scratch.ipynb gives you the end to end code and helps you develop intuition, this notebook is solely focused on training the model. So I've omitted the example implementations of various things so you can see the final code (still very much from scratch).\n",
    "\n",
    "The code was entirely developed on Databricks using a single A100 for compute. If you're running this on Databricks, you can scale this on an arbitrarily large GPU cluster with no issues in the cloud provider of your choice\n",
    "\n",
    "I chose to use mlflow (which comes pre-installed in Databricks. You can pip install easily elsewhere) as I find it helpful to track and log all the metrics necessary. This is entirely optional\n",
    "\n",
    "Please note that the implementation emphasizes readability and hackability vs performance, so there are many ways in which you could improve this. Please try and let me know \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4a58a8-bd4c-40de-a4a9-95457842db0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![mixture of experts overview](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/moe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b3daa3-3b3b-47af-b3e7-be95878f9e06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using mlflow is entirely optional. I personally like to use MLFlow to track and log everything. If you're using Databricks, it comes pre-installed.\n",
    "%pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1a3e38-8717-42ec-9bbc-71d3712c1c68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 14:06:11 INFO mlflow.tracking.fluent: Experiment with name '/MOE-AGT-1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/247206875685229266', creation_time=1745953571380, experiment_id='247206875685229266', last_update_time=1745953571380, lifecycle_stage='active', name='/MOE-AGT-1', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the necessary packages and set seed for reproducibility. For this notebook, pytorch is all you need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow setup\n",
    "import os\n",
    "#Optional\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# run this in new terminal of this jupyter notebook\n",
    "# mlflow server --host 0.0.0.0 --port 7777\n",
    "# ssh -N -L 7777:localhost:7777 -J higoyal@linux7.cs.uchicago.edu higoyal@gpu3\n",
    "remote_server_uri = \"http://0.0.0.0:7777\"  # set to your server URI, e.g. http://127.0.0.1:8080\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "experiment_name = \"/MOE-AGT-1\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.enable_system_metrics_logging()\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "# os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45143d84-28c7-463d-9fb5-e21122842600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-29 13:55:18--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‚Äòinput.txt.1‚Äô\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-04-29 13:55:18 (61.4 MB/s) - ‚Äòinput.txt.1‚Äô saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 29 13:55:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  |   00000000:19:00.0 Off |                  N/A |\n",
      "| 25%   20C    P8              7W /  250W |       2MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# this should show a GPU or else setup is waste\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde3273f-0519-4108-ba84-dfd99e020722",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Causal Scaled Dot Product Self Attention \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e435d0cf-1383-446a-9026-cd80b4266019",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![scaled dot product self attention](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/self_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7764385-26e9-4d75-9aa7-ce011023e24e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Top-k Gating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3fca4df-4c47-4e9a-98cd-08cf8ccf7726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![top k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/topk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b1f67bd-2930-4fad-9e9f-0ce2f278c4c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Noisy Top-k Gating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05b3306-b89f-4ebc-901b-f16398a925c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![noisy top-k gating](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/noisytopkgating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99b5dce-301e-4380-8263-b5cfb4136ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![sparse MoE](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/sparseMoEfinal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5f3be4-f155-4d6c-bcbd-2f88a088261f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eaf71cd-c77e-40c7-b5be-e364e91685cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#First defining hyperparameters and boiler plate code. Imports and data preparation code is repeated for convenience\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 400\n",
    "head_size = 16\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1180f7-5004-4425-87fe-9a81a17b9024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "    \n",
    "#Multi-Headed Self Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03611a92-aaa2-4e0d-9755-cba56f96c794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Expert module\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "#noisy top-k gating\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        #layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    \n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        #Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        #Adding scaled unit gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n",
    "    \n",
    "#Now create the sparse mixture of experts module\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdff2bb-092f-41c8-9a33-c84e6f8d6633",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#First create a self attention + mixture of experts block, that may be repeated several number of times \n",
    "#Copy pasting key architecture variables for clarity\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Mixture of Experts Transformer block: communication followed by computation (multi-head self attention + SparseMoE) \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
    "        # n_embed: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d32a276-d0cc-4808-90d7-62441771af44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Finally putting it all together to crease a sparse mixture of experts language model\n",
    "class SparseMoELanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head, num_experts=num_experts,top_k=top_k) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "622ba7ce-3f20-4820-8982-93f3d3b7be09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Kaiming He initialization is used here because of presence of ReLU activations in the experts. Feel free to experiment with Glorot initialization which is more commonly used in transformers. Jeremy Howard's Fastai Part 2 has an excellent lecture that implements these from scratch: https://course.fast.ai/Lessons/lesson17.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d3c057-08ee-4c1b-8013-6a88b2eadac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def kaiming_init_weights(m):\n",
    "    if isinstance (m, (nn.Linear)): \n",
    "        init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4d9525-8405-4a51-adda-661aba004e57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMoELanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 128)\n",
       "  (position_embedding_table): Embedding(32, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SparseMoELanguageModel()\n",
    "model.apply(kaiming_init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6adf1d04-e668-4d14-b691-161ea4e4dccf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I have used mlflow to track and log the metrics I care about and the training hyperparameters. The training loop in the next cell includes this mlflow code. If you prefer to just train without using mlflow, the subsequent cell has code without the mlflow code. However, I find it very convenient to track parameters and metrics, particularly when experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8968247-0d7b-4460-b96b-06743b31c55d",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 14:12:36 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.996545 M parameters\n",
      "step 0: train loss 2.4040, val loss 2.3958\n",
      "step 100: train loss 2.3262, val loss 2.3392\n",
      "step 200: train loss 2.2430, val loss 2.2710\n",
      "step 300: train loss 2.1808, val loss 2.2166\n",
      "step 400: train loss 2.1132, val loss 2.1583\n",
      "step 499: train loss 2.0672, val loss 2.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 14:17:31 INFO mlflow.tracking._tracking_service.client: üèÉ View run amazing-colt-281 at: http://0.0.0.0:7777/#/experiments/247206875685229266/runs/6deaaee068024ec39c23d0e944588913.\n",
      "2025/04/29 14:17:31 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://0.0.0.0:7777/#/experiments/247206875685229266.\n",
      "2025/04/29 14:17:31 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/04/29 14:17:31 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n",
      "2025/04/29 14:17:32 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to moe_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 14:17:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7fe855a83d90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using MLFlow\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "max_iters = 5000\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#mlflow.set_experiment(\"makeMoE\")\n",
    "with mlflow.start_run():\n",
    "    #If you use mlflow.autolog() this will be automatically logged. I chose to explicitly log here for completeness\n",
    "    params = {\"batch_size\": batch_size , \"block_size\" : block_size, \"max_iters\": max_iters, \"eval_interval\": eval_interval, \"learning_rate\": learning_rate, \"device\": device, \"eval_iters\": eval_iters, \"dropout\" : dropout, \"num_experts\": num_experts, \"top_k\": top_k }\n",
    "    mlflow.log_params(params)\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            metrics = {\"train_loss\": float(losses['train']), \"val_loss\": float(losses['val'])}\n",
    "            mlflow.log_metrics(metrics, step=iter)\n",
    "\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# Save the trained model\n",
    "model_path = \"moe_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "mlflow.log_artifact(model_path)\n",
    "mlflow.pytorch.log_model(model, artifact_path=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/26332/ipykernel_1326534/1250969649.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"moe_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# load model from local file\n",
    "model = SparseMoELanguageModel()  # Replace with your model class and args\n",
    "model.load_state_dict(torch.load(\"moe_model.pth\"))\n",
    "model.to(device)\n",
    "m = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa6e4c4-c688-4985-a3b8-e2af1f771e54",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GIUS:\n",
      "Hy yefe the to fe to by slo quty.\n",
      "\n",
      "RAUKIZENK:\n",
      "Fal rore uswnring I doven, I ral's, yot and seare dee;\n",
      "He do disee whe a the I mand hatith mad,\n",
      "I, bucall radot of wheron this hat e you-sou, cot ofagen'\n",
      "\n",
      "AhE CI IUS:\n",
      "No a concuse nonk, and to mall magae foone,\n",
      "Tooke, thess goo bave of tighere roweaen: withe,\n",
      "Pe is mans the wate be 'h Qelsit to cane.\n",
      "\n",
      "LIZRELAST:\n",
      "Firec.\n",
      "\n",
      "So somalf is boneps to manchers'ss,\n",
      "I ip yod leaeay wing, unen anucat boods the of I exhe we,\n",
      "Nobe ticke nis devenfeat yo to say.\n",
      "\n",
      "CithitnNUKETEET:\n",
      "Xe, pet toote, them dor of the aps deack:\n",
      "Ie contoos, yombrowlese cing youneriu\n",
      "Wherst gar gract\n",
      "I'e he hice oIf the hat mave theat\n",
      "My ou houstach buire Andopt todere come!\n",
      "\n",
      "GUTHAMILANT:\n",
      "Ay, bow in lock's b, to saise to?\n",
      "\n",
      "MEORG ICI:\n",
      "Ousest, mayis so-sce no modeen, hea athe your,\n",
      "you knal, a I low dothat we wilh\n",
      "Whe bat hat for me. MEStedsen the bow je RFST:\n",
      "Fory of have, he e, me the spume ghe spececakes the me\n",
      "'Mpoich, mourniny of tou my ubepornged\n",
      "Tromere of ava trewaral oned of geentit, of\n",
      "And me grome is's to uple peritinces mione menionanar\n",
      "Come het te e-to gegeeeresen mo.\n",
      "\n",
      "MPUTEMY HART:\n",
      "Your-I wite pe they to to groe. gate hense?\n",
      "\n",
      "APLIOUET:\n",
      "He viled lot hand; wore op hat bbus here ancm of.\n",
      "\n",
      "QUCLIORUCIO:\n",
      "To thine-'s of thee gack nee forsiostionest?\n",
      "\n",
      "Aarrors, or he seece, supion spemoneten's\n",
      "O, macand hat court red fotathss thene this as yow? ad and loo fe told your muo fothe you'sl:\n",
      "Of?\n",
      "O te inpen thine do wownesso paretet, not my freat icerge.\n",
      "\n",
      "AxSARME:\n",
      "No do, ichis dromiet, tickes, Go kit o'thon he a to trich mure gre himes of\n",
      "Astatede boock herst: uspelabe acttich in heeand\n",
      "As apongoor of yournce Anoulct his howers slivire\n",
      "Which yold wacte wough viry ho',\n",
      "He pronoo denet to! lord fore gure:\n",
      "Edoomfet 'Forch cou than prefe bohowe jepo;\n",
      "And rave the hOf; brom mat now theat he\n",
      "pe upape darb-arce yory arelpent hee I he?\n",
      "\n",
      "LARLAR:\n",
      "Wetiry mprionsenteces han, offey the rater;\n",
      "The gise cone hat woppat getare;\n",
      "The fat will gonces on me--is, his\n",
      "Rit kis bri\n"
     ]
    }
   ],
   "source": [
    "# generate from the model. Not great. Not too bad either\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/29 14:32:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7fe8362d3910>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.log_artifact(model_path)\n",
    "mlflow.pytorch.log_model(model, artifact_path=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "The following failures occurred while downloading one or more artifacts from http://0.0.0.0:7777/api/2.0/mlflow-artifacts/artifacts/247206875685229266/6deaaee068024ec39c23d0e944588913/artifacts:\n##### File model #####\nAPI request to http://0.0.0.0:7777/api/2.0/mlflow-artifacts/artifacts/247206875685229266/6deaaee068024ec39c23d0e944588913/artifacts/model failed with exception HTTPConnectionPool(host='0.0.0.0', port=7777): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/247206875685229266/6deaaee068024ec39c23d0e944588913/artifacts/model (Caused by ResponseError('too many 500 error responses'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6deaaee068024ec39c23d0e944588913\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# replace with actual run ID\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns:/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m m2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n",
      "File \u001b[0;32m~/AGT-MOE-zerosum/moe_venv/lib/python3.8/site-packages/mlflow/pytorch/__init__.py:684\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, dst_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03mLoad a PyTorch model from a local file or a run.\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m    predict X: 30.0, y_pred: 60.48\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m--> 684\u001b[0m local_model_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download_artifact_from_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m pytorch_conf \u001b[38;5;241m=\u001b[39m _get_flavor_configuration(model_path\u001b[38;5;241m=\u001b[39mlocal_model_path, flavor_name\u001b[38;5;241m=\u001b[39mFLAVOR_NAME)\n\u001b[1;32m    686\u001b[0m _add_code_from_conf_to_system_path(local_model_path, pytorch_conf)\n",
      "File \u001b[0;32m~/AGT-MOE-zerosum/moe_venv/lib/python3.8/site-packages/mlflow/tracking/artifact_utils.py:116\u001b[0m, in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path, lineage_header_info)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repo, ModelsArtifactRepository):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m repo\u001b[38;5;241m.\u001b[39mdownload_artifacts(\n\u001b[1;32m    112\u001b[0m         artifact_path\u001b[38;5;241m=\u001b[39martifact_path,\n\u001b[1;32m    113\u001b[0m         dst_path\u001b[38;5;241m=\u001b[39moutput_path,\n\u001b[1;32m    114\u001b[0m         lineage_header_info\u001b[38;5;241m=\u001b[39mlineage_header_info,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AGT-MOE-zerosum/moe_venv/lib/python3.8/site-packages/mlflow/store/artifact/runs_artifact_repo.py:131\u001b[0m, in \u001b[0;36mRunsArtifactRepository.download_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, artifact_path, dst_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Download an artifact file or directory to a local directory if applicable, and return a\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    local path for it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        Absolute path of the local filesystem location containing the desired artifacts.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AGT-MOE-zerosum/moe_venv/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repo.py:302\u001b[0m, in \u001b[0;36mArtifactRepository.download_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m    296\u001b[0m         template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##### File \u001b[39m\u001b[38;5;132;01m{path}\u001b[39;00m\u001b[38;5;124m #####\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{error}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    299\u001b[0m         template\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath, error\u001b[38;5;241m=\u001b[39merror, traceback\u001b[38;5;241m=\u001b[39mtracebacks[path])\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m path, error \u001b[38;5;129;01min\u001b[39;00m failed_downloads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    301\u001b[0m     )\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    303\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following failures occurred while downloading one or more\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m artifacts from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_truncate_error(failures)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         )\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst_path, artifact_path)\n",
      "\u001b[0;31mMlflowException\u001b[0m: The following failures occurred while downloading one or more artifacts from http://0.0.0.0:7777/api/2.0/mlflow-artifacts/artifacts/247206875685229266/6deaaee068024ec39c23d0e944588913/artifacts:\n##### File model #####\nAPI request to http://0.0.0.0:7777/api/2.0/mlflow-artifacts/artifacts/247206875685229266/6deaaee068024ec39c23d0e944588913/artifacts/model failed with exception HTTPConnectionPool(host='0.0.0.0', port=7777): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/247206875685229266/6deaaee068024ec39c23d0e944588913/artifacts/model (Caused by ResponseError('too many 500 error responses'))"
     ]
    }
   ],
   "source": [
    "# optional\n",
    "run_id = \"6deaaee068024ec39c23d0e944588913\"  # replace with actual run ID\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "m2 = model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             run_id                       start_time    status\n",
      "0  d2ffe7aaf0e74adc9f315be182c0eb62 2025-04-29 19:17:32.456000+00:00   RUNNING\n",
      "1  6deaaee068024ec39c23d0e944588913 2025-04-29 19:12:36.203000+00:00  FINISHED\n",
      "2  a3c09b1d0618446fa748def99a45f08c 2025-04-29 19:09:51.395000+00:00    FAILED\n"
     ]
    }
   ],
   "source": [
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id], order_by=[\"start_time DESC\"])\n",
    "print(runs_df[['run_id', 'start_time', 'status']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed96085-c292-4624-a2cc-be8aad38df79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Logging train and validation losses gives you a good indication of how the training is going. The plot shows that I probably should have stopped around 4500 steps (when the validation loss jumps up a bit)\n",
    "\n",
    "![mlflow_dash](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/mlflow_dash.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6360e1b7-94c4-4ef1-a850-9bc93f49a083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Not using MLflow\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "makeMoE_concise",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
